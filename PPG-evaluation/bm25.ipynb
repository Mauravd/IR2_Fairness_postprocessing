{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = set(stopwords.words('english'))\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "\n",
    "import json\n",
    "\n",
    "doc_file = 'corpus-subset-for-queries.jsonl.txt'\n",
    "train_file = 'TREC-Fair-Ranking-training-sample.json.txt'\n",
    "\n",
    "with open(doc_file, 'r', encoding='utf-8') as f:\n",
    "    doc_data = json.loads('[{}]'.format(','.join(f)))\n",
    "\n",
    "with open(train_file, 'r', encoding='utf-8') as f:\n",
    "    train_data = json.loads('[{}]'.format(','.join(f)))\n",
    "\n",
    "\n",
    "doc_id_idx = {x['id']:i for i, x in enumerate(doc_data)}\n",
    "\n",
    "labels = ['title', 'paperAbstract', 'fieldsOfStudy']\n",
    "\n",
    "d = {}  \n",
    "for query in train_data:\n",
    "    q = query['query']\n",
    "    d[q] = {}\n",
    "    for doc in query['documents']:\n",
    "        try:\n",
    "            doc_id = doc['doc_id']\n",
    "            s = []\n",
    "            data = doc_data[doc_id_idx[doc_id]]\n",
    "            s.append(data[labels[0]])\n",
    "            s.append(data[labels[1]])\n",
    "            if len(data[labels[2]]) > 0:\n",
    "                s.append(' '.join(data[labels[2]]))\n",
    "            d[q][doc_id] = ' '.join(s)\n",
    "        except KeyError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "        Tokenizes the input text. Use the WordPunctTokenizer\n",
    "        Input: text - a string\n",
    "        Output: a list of tokens\n",
    "    \"\"\"\n",
    "    tk = nltk.tokenize.WordPunctTokenizer()   \n",
    "    return tk.tokenize(text)\n",
    "\n",
    "def stem_token(token):\n",
    "    \"\"\"\n",
    "        Stems the given token using the PorterStemmer from the nltk library\n",
    "        Input: a single token\n",
    "        Output: the stem of the token\n",
    "    \"\"\"\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    return stemmer.stem(token)\n",
    "\n",
    "#### Putting it all together\n",
    "def process_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = []\n",
    "    for token in tokenize(text):\n",
    "        token = token.lower()\n",
    "        if token in stop_words:\n",
    "            continue\n",
    "        token = stem_token(token)\n",
    "        tokens.append(token)\n",
    "\n",
    "    return tokens\n",
    "#### \n",
    "\n",
    "def process_docs(documents):\n",
    "    processed_docs = []\n",
    "    for doc_id in documents:\n",
    "        processed_docs.append((doc_id, process_text(documents[doc_id])))\n",
    "    return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_df(documents):\n",
    "    \"\"\"\n",
    "        Compute the document frequency of all terms in the vocabulary\n",
    "        Input: A list of documents\n",
    "        Output: A dictionary with {token: document frequency (int)}\n",
    "    \"\"\"\n",
    "\n",
    "    df = defaultdict(int)\n",
    "\n",
    "    for tokens in documents:\n",
    "        used_tokens = set()\n",
    "\n",
    "        for token in tokens:\n",
    "            if token not in used_tokens:\n",
    "                df[token] += 1\n",
    "                used_tokens.add(token)\n",
    "    \n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tf_index(documents):\n",
    "    \"\"\"\n",
    "        Build an inverted index (with counts). The output is a dictionary which takes in a token\n",
    "        and returns a list of (doc_id, count) where 'count' is the count of the 'token' in 'doc_id'\n",
    "        Input: a list of documents - (doc_id, tokens) \n",
    "        Output: An inverted index implemented within a pyhton dictionary: [token] -> [(doc_id, token_count)]\n",
    "    \"\"\"\n",
    "    \n",
    "    tf_index = defaultdict(list)\n",
    "    \n",
    "    token_set = set()\n",
    "\n",
    "    for doc_id, tokens in documents:\n",
    "        for token in tokens:\n",
    "            token_set.add(token)\n",
    "\n",
    "    for doc_id, tokens in documents:\n",
    "        used_tokens = set()\n",
    "        counter = Counter(tokens)\n",
    "\n",
    "        for token in tokens:\n",
    "            if token not in used_tokens:\n",
    "                tf_index[token].append((doc_id, counter[token]))\n",
    "                used_tokens.add(token)\n",
    "        \n",
    "        for token in token_set:\n",
    "            if token not in used_tokens:\n",
    "                tf_index[token].append((doc_id, 0))\n",
    "                used_tokens.add(token)\n",
    "\n",
    "    return tf_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Document length for normalization\n",
    "def doc_lengths(documents):\n",
    "    lengths = {doc_id : len(doc) for (doc_id, doc) in documents}\n",
    "    return lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_search(query, documents):\n",
    "    \"\"\"\n",
    "        Perform a search over all documents with the given query using BM25. Use k_1 = 1.5 and b = 0.75\n",
    "        Note #1: You have to use the `get_index` (and `get_doc_lengths`) function created in the previous cells\n",
    "        Note #2: You might have to create some variables beforehand and use them in this function\n",
    "        \n",
    "        Input: \n",
    "            query - a (unprocessed) query\n",
    "            index_set - the index to use\n",
    "        Output: a list of (document_id, score), sorted in descending relevance to the given query \n",
    "    \"\"\"\n",
    "    \n",
    "    documents = process_docs(documents)\n",
    "    processed_query = process_text(query)\n",
    "\n",
    "    index = build_tf_index(documents)\n",
    "    df = compute_df([d[1] for d in documents])\n",
    "    doc_lens = doc_lengths(documents)\n",
    "    \n",
    "    k_1 = 1.5\n",
    "    b = 0.75\n",
    "\n",
    "    N = len(doc_lens)\n",
    "    avgdl = sum(doc_lens.values()) / N\n",
    "\n",
    "\n",
    "    # List of [doc_id, score] pairs\n",
    "    scores = []\n",
    "    \n",
    "    # Keeps track of idx of document rank in the scores list\n",
    "    doc_id_pos = dict()\n",
    "\n",
    "    for token in processed_query:\n",
    "        for doc_id, count in index[token]:\n",
    "            count = float(count)\n",
    "\n",
    "            idf = np.log(N / df[token])\n",
    "            tf = count\n",
    "\n",
    "            score = idf * ((k_1 + 1) * tf) / (k_1 * ((1 - b) + b * doc_lens[doc_id] / avgdl) + tf)\n",
    "\n",
    "            if doc_id not in doc_id_pos:\n",
    "                scores.append([doc_id, score])\n",
    "                doc_id_pos[doc_id] = len(scores) - 1\n",
    "            else:\n",
    "                scores[doc_id_pos[doc_id]][1] += score\n",
    "            \n",
    "    \n",
    "    # convert to list of tuples\n",
    "    scores = [(doc_id, score) for doc_id, score in scores]\n",
    "\n",
    "    return sorted(scores, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drogenzubereitung\n",
      "робустова\n",
      "夏忠军\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for query in d.keys():\n",
    "    docs = d[query]\n",
    "    ranking = bm25_search(query, docs)\n",
    "    # print(ranking)\n",
    "    if not ranking:\n",
    "        print(query)\n",
    "    # i += 1\n",
    "    # if (i ==10):\n",
    "    #     break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22b25d8d397e47ff6543a63909c61a8509f09ca9a2933d43f6e2d0039531da57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
